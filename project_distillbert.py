# -*- coding: utf-8 -*-
"""project-distillbert

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/12er_9qHRbPVfXoHPiDhObJTGRbfhjMmM

# Imports and Model
"""

!pip install jsonlines
!pip install tqdm

import torch
import jsonlines
from tqdm import tqdm
from transformers import DistilBertTokenizer, DistilBertForMaskedLM
from transformers import AdamW
from transformers import T5EncoderModel, T5Tokenizer, AutoTokenizer, AutoModelForCausalLM

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# Masked generation model and tokenizer
tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')
model = DistilBertForMaskedLM.from_pretrained("distilbert-base-uncased").to(device)

def predict_top_k(texts, model, tokenizer, k):

  # Tokenize the input text
  inputs = tokenizer(texts, return_tensors='pt', padding=True)
  inputs = {key: value.to(device) for key, value in inputs.items()}
  mask_indices = torch.where(inputs['input_ids'] == tokenizer.mask_token_id)[1].to(device)

  # Predict the masked token
  predictions = model(**inputs).logits

  batch_indices = torch.arange(predictions.size(0))
  topk_token_ids = predictions[batch_indices, mask_indices].topk(k).indices

  result = []

  # Decode each token ID to its corresponding word
  for i, token_ids in enumerate(topk_token_ids):
    # Now token_ids is a single-dimensional tensor for the current masked token
    topk_words = tokenizer.decode(token_ids, skip_special_tokens=True).split()
    result.append(topk_words)

  return result

sentences = [
    "The capital of France is [MASK].",
    "At the beach, [MASK] might protect you from the sun.",
    "At Age [MASK] It Become Embarrassing To Still Live With Your Parents (Numeric Only)? "
]

result = predict_top_k(sentences, model, tokenizer, 10)
print(result)

"""# dataloader

"""

class loadquestion():
  def __init__(self, file_path):
    self.file_path = file_path
    self.data = jsonlines.open(self.file_path)
    dataset = []
    with jsonlines.open(file_path) as f:
        for line in f.iter():
            dataset.append(line)
    self.dataset = dataset
    self.questions = [self.dataset[i]['question']['normalized'] for i in range(len(self.dataset))]
  def __len__(self):
    return len(self.dataset)
  def __getitem__(self,idx):
    return self.dataset[idx]
  def get_questions(self):
    [
    [key for key, count in self.dataset[i]['answers']['raw'].items()]
    for i in range(len(self.dataset))
    ]
  def get_answers(self):
    return self.answers
  def get_masked_questions(self):
    keyword_bank = ['who', 'what', 'why', 'name', 'how', 'where', 'when', "which"]
    masked = []
    for sentence in self.questions:
      sentence1 = sentence
      if '?' in sentence1:
        sentence1 = sentence.replace('?', '.')
      if(sentence1.startswith('name')):
        masked.append((sentence1.replace('name', '[MASK] is', 1)))
        continue
      elif('__' in sentence1):
        masked.append(sentence1.replace('__', '[MASK] is', 1))
        continue
      elif(sentence1.startswith('tell me')):
        masked.append((sentence1.replace('tell me', '[MASK] is', 1)))
        continue

      elif(sentence1.startswith('give me')):
        masked.append((sentence1.replace('give me', '[MASK] is', 1)))
        continue
      # elif(sentence.startswith('besides')): #take care if name is not in
      #   #print(sentence.replace('name', '[MASK] is', 1))
      #    continue
      cont = False
      for word in keyword_bank:
        if word == 'name' and word in sentence1:
          masked.append(sentence1.replace(word, '[MASK] is', 1))
          cont = True
          break
        elif(word in sentence):
          masked.append(sentence1.replace(word, '[MASK]', 1))
          cont = True
          break
      if not cont:
        masked.append(sentence1[:-1] + ' [MASK]')

    return masked

"""# Produce words"""

train_loader = loadquestion('train.jsonl')

train_questions = train_loader.get_masked_questions()

batch_size = 32

set_words = ['I', 'you','my','mine','myself','we','us','our','ours','ourselves','you',
  'your','yourselves','he','him', 'his', 'himself','she','her','herself','it','its',
  'itself','they','them','their','theirs','themself','they','them','their','theirs',
  'themselves', 'here', 'oh', 'a', 'an', 'such', 'whatever', 'each', 'where', 'what', 'why',
  'how', 'who', 'whom','when', 'that','those', 'for', ':', 'because', 'cause', 'there', 'it',
  'this', 'here', 'or', ';', 'these', 'the', 'some', 'nor', 'so', '"', 'but', 'and', 'now'
  'because.', 'here?', 'yourself!', 'there,', 'you.', 'which', 'probably']

results = []

loop = tqdm(total = len(train_questions) / batch_size, position=0, leave=False)
for i in range(0, len(train_questions), batch_size):
  batch_q = train_questions[i:i+batch_size]
  batch_results = predict_top_k(batch_q, model, tokenizer, k=20)
  #print(batch_q)
  #print(batch_results)
  for j in range(0, len(batch_results)):
    answer_list = []
    k = 0
    for word in batch_results[j]:
      if word not in set_words and k < 10:
        k += 1
        answer_list.append(word)
    #print(question_list)
    results.append(answer_list)
  loop.update(1)

print(train_questions[0])
print(results[0])

from itertools import product
import nltk
import numpy as np
from nltk.corpus import wordnet as wn
nltk.download('wordnet')

def compute_wordnet_for_batch(list1, list2):
    loss = []
    for sub_list1, sub_list2 in zip(list1, list2):  # Assuming you want to compare corresponding lists
        total_similarity = 0
        count = 0
        for word1, word2 in product(sub_list1, sub_list2):  # Compare every word in sub_list1 with every word in sub_list2
            synsets1 = wn.synsets(word1)
            synsets2 = wn.synsets(word2)
            if not synsets1 or not synsets2:
                continue
            synset1 = synsets1[0]
            synset2 = synsets2[0]
            similarity = synset1.wup_similarity(synset2)
            if similarity is not None and not np.isnan(similarity):  # Check for NaN explicitly
                total_similarity += similarity
                count += 1
        if count > 0:
            loss.append(total_similarity / count)
        else:
            loss.append(0)  # Append 0 if no valid similarities were found

    return torch.tensor(loss, requires_grad=True).mean()

train_dataset = []
with jsonlines.open('train.jsonl') as f:
    for line in f.iter():
        train_dataset.append(line)

train_answers_normalized = [
    [key for key, count in train_dataset[i]['answers']['raw'].items()]
    for i in range(len(train_dataset))
]
j = 0
loss = [];
for i in range(0, len(train_questions), batch_size):
    given_answer = train_answers_normalized[i:i+batch_size]
    compute_answer = results[j]
    j += 1
    loss.append(compute_wordnet_for_batch(given_answer, compute_answer))

print(loss)

"""# Updating generation model with a reward model"""

# Models
eval_tokenizer = transformers.AutoTokenizer.from_pretrained('liujch1998/vera')
eval_model = transformers.T5EncoderModel.from_pretrained('liujch1998/vera')
eval_model.D = eval_model.shared.embedding_dim

def get_vera_score(batch):
  linear = torch.nn.Linear(eval_model.D, 1, dtype=eval_model.dtype)
  linear.weight = torch.nn.Parameter(eval_model.shared.weight[32099, :].unsqueeze(0))
  linear.bias = torch.nn.Parameter(eval_model.shared.weight[32098, 0].unsqueeze(0))


  eval_model.eval()
  t = eval_model.shared.weight[32097, 0].item() # temperature for calibration
  # print(batch)
  scores = []
  for statement in batch:
    print(statement)
    input_ids = eval_tokenizer.batch_encode_plus([statement], return_tensors='pt', padding='longest', truncation='longest_first', max_length=128).input_ids
    with torch.no_grad():
        output = eval_model(input_ids)
        last_hidden_state = output.last_hidden_state
        hidden = last_hidden_state[0, -1, :]
        logit = linear(hidden).squeeze(-1)
        logit_calibrated = logit / t
        score_calibrated = logit_calibrated.sigmoid()
        # score_calibrated is Vera's final output plausibility score
        scores.append(score_calibrated.item())
        print(score_calibrated.item())
  # print(scores)
  return torch.tensor(scores).mean()

# Train for one epoch
def train(lr=0.01, batch_size=16):
  optimizer = AdamW(model.parameters(), lr=lr)

  losses = []
  batches = len(train_questions) // batch_size
  loop = tqdm(total=batches, position=0, leave=False)
  for i in range(0, len(train_questions), batch_size):
    batch_q = train_questions[i:i+batch_size]

    optimizer.zero_grad()

    predictions = predict_top_k(batch_q, model, tokenizer, k=1)

    for j in range(len(batch_q)):
      batch_q[j] = batch_q[j].replace('[MASK]', predictions[j][0], 1)

    loss = get_vera_score(batch_q)
    losses.append(loss.item())

    loss.backward()
    optimizer.step()

    loop.set_description(f"Loss: {loss.item():.4f}")
    loop.update(1)

  return losses

epoch = 5
lr = 0.001
batch_size = 16

total_losses = []

for i in range(epoch):
  losses = train(lr=lr, batch_size=batch_size)
  total_losses.extend(losses)

"""Dev Set"""

dev_loader = loadquestion('dev.scraped.jsonl')
dev_questions = dev_loader.get_masked_questions()

dev_results = []

loop = tqdm(total = len(dev_questions) / batch_size, position=0, leave=False)
for i in range(0, len(dev_questions), batch_size):
  batch_q = dev_questions[i:i+batch_size]
  batch_results = predict_top_k(batch_q, model, tokenizer, k=20)
  #print(batch_q)
  #print(batch_results)
  for j in range(0, len(batch_results)):
    answer_list = []
    k = 1
    for word in batch_results[j]:
      if word not in set_words and k < 10:
        k += 1
        answer_list.append(word)
    #print(question_list)
    dev_results.append(answer_list)
  loop.update(1)

dev_dataset = []
with jsonlines.open('dev.scraped.jsonl') as f:
    for line in f.iter():
        dev_dataset.append(line)

dev_answers_normalized = [
    [key for key, count in dev_dataset[i]['answers']['raw'].items()]
    for i in range(len(dev_dataset))
]
j = 0
dev_loss = [];
for i in range(0, len(dev_questions), batch_size):
    given_answer = dev_answers_normalized[i:i+batch_size]
    compute_answer = dev_results[j]
    j += 1
    dev_loss.append(compute_wordnet_for_batch(given_answer, compute_answer))

print(dev_loss)

"""Test"""

test_loader = loadquestion('test.questions.jsonl')
test_questions = test_loader.get_masked_questions()

test_results = []

loop = tqdm(total = len(test_questions) / batch_size, position=0, leave=False)
for i in range(0, len(test_questions), batch_size):
  batch_q = test_questions[i:i+batch_size]
  batch_results = predict_top_k(batch_q, model, tokenizer, k=20)
  #print(batch_q)
  #print(batch_results)
  for j in range(0, len(batch_results)):
    answer_list = []
    k = 1
    for word in batch_results[j]:
      if word not in set_words and k < 10:
        k += 1
        answer_list.append(word)
    #print(question_list)
    test_results.append(answer_list)
  loop.update(1)

import json

test_labels = []
with jsonlines.open('test.questions.jsonl') as f:
    for item in f.iter():
        test_labels.append(item["metadata"]["id"])

with open('answer.json', 'w') as json_file:
    for idx, answers in enumerate(test_results):
        label = test_labels[idx]  # Assuming test_labels contains the labels
        json_string = json.dumps({label: answers})
        json_file.write(json_string + '\n')