# -*- coding: utf-8 -*-
"""Final Project ProtoQA

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/16v7D7V1ZgJd3SZEQnqX02T3Llbc22ltm
"""

!pip install transformers datasets torch
!pip install pandas
!pip install json
!pip install datasets
!pip install jsonlines

from transformers import AutoTokenizer, AutoModelForCausalLM, AutoModelForTokenClassification, AutoTokenizer
from transformers import AdamW
from datasets import Dataset
import torch
import json

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# Initialize the GPT model and tokenizer
gpt_model_name = 'gpt2-medium'
gpt_tokenizer = AutoTokenizer.from_pretrained(gpt_model_name, padding_side='left')
gpt_tokenizer.pad_token = gpt_tokenizer.eos_token
gpt_model = AutoModelForCausalLM.from_pretrained(gpt_model_name).to(device)
gpt_model.config.pad_token_id = gpt_tokenizer.eos_token_id

# Initialize the NER model and tokenizer
ner_model_name = 'dbmdz/bert-large-cased-finetuned-conll03-english'
ner_tokenizer = AutoTokenizer.from_pretrained(ner_model_name)
ner_model = AutoModelForTokenClassification.from_pretrained(ner_model_name).to(device)

from nltk.corpus import wordnet as wn
from itertools import product
from tqdm import tqdm
import jsonlines

train_dataset = []
with jsonlines.open('train.jsonl') as f:
    for line in f.iter():
        train_dataset.append(line)
print(train_dataset[0])
zero = train_dataset[0]
print(zero['question'])

train_questions_normalized = [train_dataset[i]['question']['normalized'] for i in range(len(train_dataset))]


# for i in range(len(train_questions_normalized)):
#     train_questions_normalized[i] = train_questions_normalized[i][:len(train_questions_normalized[i])-1] + " is"
#     train_questions_normalized[i] = train_questions_normalized[i].replace("name something", "what")
# print(train_questions_normalized[0])
train_answers_normalized = [
    [key for key, count in train_dataset[i]['answers']['raw'].items()]
    for i in range(len(train_dataset))
]

def compute_loss(batch, generation_tokenizer, generation_model, selection_tokenizer, selection_model, max_new_tokens=20):
  # Tokenize the batch -> list of strings (of questions)
  question_tokens = generation_tokenizer(batch, padding=True, return_tensors="pt").to(device)

  # Generate
  output_tokens = generation_model.generate(input_ids=question_tokens["input_ids"], max_new_tokens=max_new_tokens, pad_token_id=generation_tokenizer.eos_token_id, do_sample=True)

  # Decode the generated texts and store them in a list.
  generated = [generation_tokenizer.decode(output_tokens[i], skip_special_tokens=True) for i in range(len(output_tokens))]

  # print(generated)

  selection_tokens = selection_tokenizer(generated, padding=True, truncation=True, return_tensors='pt').to(device) # padding=True, truncation=True

  outputs = selection_model(**selection_tokens).logits
  # print(outputs[:5])
  predictions = torch.argmax(outputs, dim=2)
  # print(predictions[:5])

  # tokens = generation_tokens.tokens()
  # keywords = [tokens[i] for i, prediction in enumerate(predictions[0].numpy()) if prediction != 0]
  # print(keywords)

  keywords = []
  for i, tokens in enumerate(selection_tokens['input_ids']):
      tokenized_string = selection_tokenizer.convert_ids_to_tokens(tokens)
      sentence_keywords = [tokenized_string[j] for j in range(len(tokenized_string)) if predictions[i][j].item() != 0]
      keywords.append(sentence_keywords)

  print("Keywords:", keywords)

# def compute_wordnet_for_batch(list1, list2):
#     loss = []
#     for i in range(len(list1)):
#       total_similarity = 0
#       count = 0
#       # print(len(list1))
#       # print(len(list2))
#       for word1, word2 in product(list1, list2):
#           synsets1 = wn.synsets(word1[i])
#           synsets2 = wn.synsets(word2[i])

#           print(synsets1)
#           print(synsets2)

#           # Skip if no synset is found for either word
#           if not synsets1 or not synsets2:
#               continue

#           # Use the first synset for each word
#           synset1 = synsets1[0]
#           synset2 = synsets2[0]

#           # Compute the Wu and Palmer similarity
#           similarity = synset1.wup_similarity(synset2)

#           # Skip if similarity is None
#           if similarity is not None:
#               total_similarity += similarity
#               count += 1

#           # print(total_similarity / count)

#           if count == 0:
#               loss.append(0)  # Return 0 to avoid division by zero
#           else:
#               loss.append(total_similarity / count)
#       # print(loss)
#       return torch.tensor(loss, requires_grad=True).mean()
import numpy as np

def compute_wordnet_for_batch(list1, list2):
    loss = []
    for sub_list1, sub_list2 in zip(list1, list2):  # Assuming you want to compare corresponding lists
        total_similarity = 0
        count = 0
        for word1, word2 in product(sub_list1, sub_list2):  # Compare every word in sub_list1 with every word in sub_list2
            synsets1 = wn.synsets(word1)
            synsets2 = wn.synsets(word2)
            if not synsets1 or not synsets2:
                continue
            synset1 = synsets1[0]
            synset2 = synsets2[0]
            similarity = synset1.wup_similarity(synset2)
            if similarity is not None and not np.isnan(similarity):  # Check for NaN explicitly
                total_similarity += similarity
                count += 1
        if count > 0:
            loss.append(total_similarity / count)
        else:
            loss.append(0)  # Append 0 if no valid similarities were found

    return torch.tensor(loss, requires_grad=True).mean()

losses = train()

print(losses)
from matplotlib import pyplot as plt
plt.plot(losses)
plt.xlabel('batches')
plt.ylabel('train loss')
plt.title('Train loss vs. batches')

def compute_loss(batch, generation_tokenizer, generation_model, selection_tokenizer, selection_model, max_new_tokens=20):
    generation_model.to(device)
    selection_model.to(device)

    questions = batch['questions']
    answers = batch['answers']

    # Tokenize the batch -> list of strings (of questions)
    question_tokens = generation_tokenizer(questions, padding=True, return_tensors="pt").to(device)

    # Generate responses
    output_tokens = generation_model.generate(input_ids=question_tokens["input_ids"], max_new_tokens=max_new_tokens, pad_token_id=generation_tokenizer.eos_token_id, do_sample=True)

    # Decode the generated texts and store them in a list.
    generated_texts = [generation_tokenizer.decode(token, skip_special_tokens=True) for token in output_tokens]

    # Separate the generated answer from the question for NER processing
    # Assuming the generated text includes the question, we need to remove the question part.
    # This approach may vary depending on how your generation model works.
    answers_only = [gen[len(ques):] for ques, gen in zip(questions, generated_texts)]

    # print("Generated Answers:", answers_only)

    # Tokenize generated answers for NER
    selection_tokens = selection_tokenizer(answers_only, padding=True, truncation=True, return_tensors='pt').to(device)

    # Get NER predictions for answers only
    outputs = selection_model(**selection_tokens).logits
    predictions = torch.argmax(outputs, dim=2)

    keywords = []
    for i, tokens in enumerate(selection_tokens['input_ids']):
        tokenized_string = selection_tokenizer.convert_ids_to_tokens(tokens)
        sentence_keywords = [tokenized_string[j] for j in range(len(tokenized_string))] # predictions[i][j].item() != selection_tokenizer.pad_token_id and  if predictions[i][j].item() != 0
        keywords.append(sentence_keywords)

    # print("Keywords from Answers:", keywords)

    return compute_wordnet_for_batch(keywords, answers)

# def train(model_name, lr=1e-4, batch_size=32, debug=False):

#   gpt_optimizer = AdamW(gpt_model.parameters(), lr=lr)
#   ner_optimizer = AdamW(ner_model.parameters(), lr=lr)

#   losses = []
#   rewards = []
#   batches = len(train_questions_normalized) // batch_size
#   loop = tqdm(total=batches, position=0, leave=False)
#   for i in range(0, len(train_questions_normalized), batch_size):
#     # TODO: get the current batch of data.
#     batch_q = train_questions_normalized[i:i+batch_size]
#     batch_a = train_answers_normalized[i:i+batch_size]
#     batch = {'question': batch_q, 'answer': batch_a}

#     # TODO: compute reward of the batch using compute_reward_for_mini_batch().
#     # Hint: you need to detach the reward tensor to break gradient updates.
#     word_net = compute_loss(batch, gpt_tokenizer, gpt_model, ner_tokenizer, ner_model, max_new_tokens=20)

#     # TODO: compute REINFORCE loss.
#     # log_probs, loss = compute_reinforce_loss(reward, output_tokens, model)

#     # TODO: Compute kl divergence, and modify loss accordingly with alpha.
#     # You will complete this for section 3.

#     # TODO: compute gradients and update parameters using optimizer.
#     optimizer.zero_grad()
#     loss.backward()
#     optimizer.step()
#     # TODO: add mean reward, loss, and kl_divergence to their corresponding lists,
#     # when appropriate.
#     losses.append(word_net.item())

#     loop.set_description(f"Loss: {loss.item():.4f}")
#     loop.update(1)

def train(lr=1e-4, batch_size=32, debug=False):

  gpt_optimizer = AdamW(gpt_model.parameters(), lr=lr)
  ner_optimizer = AdamW(ner_model.parameters(), lr=lr)

  global model, orig_model, tokenizer, reward_model, optimizer
  global train_texts
  losses = []
  batches = len(train_questions_normalized) // batch_size
  loop = tqdm(total=batches, position=0, leave=False)
  for i in range(0, len(train_questions_normalized), batch_size):
    # TODO: get the current batch of data.
    batch_q = train_questions_normalized[i:i+batch_size]
    batch_a = train_answers_normalized[i:i+batch_size]
    batch = {'questions': batch_q, 'answers': batch_a}

    # TODO: compute reward of the batch using compute_reward_for_mini_batch().
    # Hint: you need to detach the reward tensor to break gradient updates.
    loss = compute_loss(batch, gpt_tokenizer, gpt_model, ner_tokenizer, ner_model, max_new_tokens=20)

    # TODO: compute REINFORCE loss.
    # log_probs, loss = compute_reinforce_loss(reward, output_tokens, model)

    # TODO: Compute kl divergence, and modify loss accordingly with alpha.
    # You will complete this for section 3.

    # TODO: compute gradients and update parameters using optimizer.
    if (i/batches % 2) == 0:
      gpt_optimizer.zero_grad()
      loss.backward()
      gpt_optimizer.step()
    else:
      ner_optimizer.zero_grad()
      loss.backward()
      ner_optimizer.step()
    # TODO: add mean reward, loss, and kl_divergence to their corresponding lists,
    # when appropriate.
    losses.append(loss.item())
    loop.set_description(f"Loss: {loss.item():.4f}")
    loop.update(1)

  return losses

import nltk
nltk.download('wordnet')

# Test the compute loss function
batch = ['Name a place children go that can be hotbed of germs. ', 'Name something a teenager will not leave the house without. ', 'Name something everybody knows about Cinderella. ']

losses = train()

def generate_and_refine_answer(question, generation_tokenizer, generation_model, selection_tokenizer, selection_model, device, max_new_tokens=20):
    # Ensure the models are in evaluation mode and on the correct device
    generation_model.eval()
    generation_model.to(device)
    selection_model.eval()
    selection_model.to(device)

    # Tokenize the question
    question_tokens = generation_tokenizer(question, return_tensors="pt", padding=True).to(device)

    # Generate a response
    output_tokens = generation_model.generate(input_ids=question_tokens["input_ids"],
                                              max_length=question_tokens["input_ids"].shape[1] + max_new_tokens,
                                              pad_token_id=generation_tokenizer.eos_token_id,
                                              do_sample=True)

    # Decode the generated text
    generated_text = generation_tokenizer.decode(output_tokens[0], skip_special_tokens=True)

    # Tokenize the generated answer for the selection model
    answer_tokens = selection_tokenizer(generated_text, return_tensors='pt', padding=True, truncation=True).to(device)

    # Get predictions from the selection model
    with torch.no_grad():  # Ensure no gradients are computed in evaluation mode
        outputs = selection_model(**answer_tokens)
        predictions = torch.argmax(outputs.logits, dim=2)

    # Assuming the selection model is used for some form of extraction or refinement,
    # here we simply return the raw generated text and the selection model's predictions.
    # Depending on the selection model's role, you might refine this further.
    return generated_text, predictions

# Example usage
question = "At The Beach, Name Something That Might Protect You From Sun."
generated_answer, selection_predictions = generate_and_refine_answer(question, gpt_tokenizer, gpt_model, ner_tokenizer, ner_model, device)
print("Generated Answer:", selection_predictions)
# The handling of selection_predictions depends on the specific output format and purpose of the selection model.

# Parse data

def preprocess_data(data):
    questions = []
    answers = []
    for cluster_key, cluster_value in data['answers']['clusters'].items():
        question = data['question']['normalized']
        for answer in cluster_value['answers']:
            questions.append(f"question: {question} answer:")
            answers.append(answer)
    return questions, answers

def process_json(file_path):
    all_questions = []
    all_answers = []
    with open(file_path, 'r', encoding='utf-8') as file:
        for line in file:
            data = json.loads(line)
            questions, answers = preprocess_data(data)
            all_questions.extend(questions)
            all_answers.extend(answers)
    return all_questions, all_answers

file_path = 'train.jsonl'
questions, answers = process_json(file_path)
# print(questions[:10])
# print(answers[:10])

# Your data structured as provided in the example
data = {
    "metadata": {
        "id": "train_q0",
        "source": "https://www.familyfeudinfo.com"
    },
    "question": {
        "original": "At The Beach, Name Something That Might Protect You From Sun.",
        "normalized": "at the beach, name something that might protect you from sun."
    },
    "answers": {
        "raw": {
            "umbrella": 38,
            "sunscreen": 36,
            "sun hat": 14,
            "sunglasses": 5,
            "cover up": 3,
            "shade": 3
        },
        "clusters": {
            "train_q0.0": {"count": 38, "answers": ["umbrella"]},
            "train_q0.1": {"count": 36, "answers": ["sunscreen"]},
            "train_q0.2": {"count": 14, "answers": ["sun hat"]},
            "train_q0.3": {"count": 5, "answers": ["sunglasses"]},
            "train_q0.4": {"count": 3, "answers": ["cover up"]},
            "train_q0.5": {"count": 3, "answers": ["shade"]}
        }
    },
    "num": {
        "answers": 99,
        "clusters": 6
    }
}

# Preprocess the data into a format suitable for T5
def preprocess_data(data):
    questions = []
    answers = []
    for cluster_key, cluster_value in data['answers']['clusters'].items():
        question = data['question']['normalized']
        for answer in cluster_value['answers']:
            questions.append(f"question: {question} answer:")
            answers.append(answer)
    return questions, answers

questions, answers = preprocess_data(data)

# Convert to Hugging Face's Dataset
dataset = Dataset.from_dict({'input_text': questions, 'target_text': answers})
dataset = dataset.train_test_split(test_size=0.1)  # Splitting dataset for training and testing

# Tokenizing the inputs and labels
tokenizer = T5Tokenizer.from_pretrained('t5-small')

def tokenize_function(examples):
    model_inputs = tokenizer(examples['input_text'], padding="max_length", truncation=True, max_length=128)
    with tokenizer.as_target_tokenizer():
        labels = tokenizer(examples['target_text'], padding="max_length", truncation=True, max_length=30)
    model_inputs["labels"] = labels["input_ids"]
    return model_inputs

tokenized_datasets = dataset.map(tokenize_function, batched=True)

# Load the T5 model
model = T5ForConditionalGeneration.from_pretrained('t5-small')

# Training arguments
training_args = TrainingArguments(
    output_dir="./results",
    num_train_epochs=3,
    per_device_train_batch_size=8,
    per_device_eval_batch_size=8,
    warmup_steps=500,
    weight_decay=0.01,
    logging_dir="./logs",
    logging_steps=10,
)

# Initialize Trainer
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_datasets['train'],
    eval_dataset=tokenized_datasets['test']
)

# Start training
trainer.train()

from transformers import BertTokenizerFast, BertForTokenClassification
from transformers import Trainer, TrainingArguments

tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')
model = BertForTokenClassification.from_pretrained('bert-base-uncased', num_labels=num_labels)

# Tokenize the input
train_encodings = tokenizer(train_texts, is_split_into_words=True, return_offsets_mapping=True, padding=True, truncation=True)
val_encodings = tokenizer(val_texts, is_split_into_words=True, return_offsets_mapping=True, padding=True, truncation=True)

# Adjust the labels
# You need to align the labels with the tokenizer's output, taking care to set labels for subword tokens to -100 so they are not considered during loss calculation.

# Define a custom dataset
class NERDataset(torch.utils.data.Dataset):
    def __init__(self, encodings, labels):
        self.encodings = encodings
        self.labels = labels

    def __getitem__(self, idx):
        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}
        item['labels'] = torch.tensor(self.labels[idx])
        return item

    def __len__(self):
        return len(self.labels)

train_dataset = NERDataset(train_encodings, train_labels)
val_dataset = NERDataset(val_encodings, val_labels)

# Set up training arguments
training_args = TrainingArguments(
    output_dir='./results',          # output directory
    num_train_epochs=3,              # total number of training epochs
    per_device_train_batch_size=16,  # batch size per device during training
    per_device_eval_batch_size=64,   # batch size for evaluation
    warmup_steps=500,                # number of warmup steps for learning rate scheduler
    weight_decay=0.01,               # strength of weight decay
    logging_dir='./logs',            # directory for storing logs
    logging_steps=10,
)

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=val_dataset
)

# Train the model
trainer.train()

def generate_answer(question):
    # Encode the question and generate an answer
    # f"question: {question} context: "
    input_ids = t5_tokenizer.encode(question, return_tensors="pt")
    outputs = t5_model.generate(input_ids, max_length=50, num_beams=5, early_stopping=True)

    # Decode and return the answer
    return t5_tokenizer.decode(outputs[0], skip_special_tokens=True)

def extract_keywords(text):
    # Tokenize text and predict NER tags
    inputs = ner_tokenizer(text, return_tensors="pt")
    outputs = ner_model(**inputs).logits
    print(outputs[:5])
    predictions = torch.argmax(outputs, dim=2)
    print(predictions[:5])


    # Decode and extract keywords
    tokens = inputs.tokens()
    keywords = [tokens[i] for i, prediction in enumerate(predictions[0].numpy()) if prediction != 0]  # Ignore 'O' tags
    return keywords

# Example question
question = "We surveyed 100 women...Name something a man can do better than a woman can. "

# Generate answer
answer = generate_answer(question)
print(f"Answer: {answer}")
# print(type(answer))
# answer = "After A Week Of Camping, What Luxury Oh Home Are You Most Excited To Have Again?"
# print(type(answer))

# Extract keywords
keywords = extract_keywords(answer)
print(f"Keywords: {keywords}")

from transformers import RobertaTokenizer

# Load RoBERTa tokenizer
tokenizer = RobertaTokenizer.from_pretrained('roberta-base')

# Tokenize words
tokens_quick = tokenizer.tokenize("quick")
tokens_lazy = tokenizer.tokenize("lazy")

print(f"'quick' tokenized: {tokens_quick}")
print(f"'lazy' tokenized: {tokens_lazy}")